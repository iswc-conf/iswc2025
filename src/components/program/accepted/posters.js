const posters = [
    {
        id: 'P1',
        title: "SLM-as-a-Judge with Attention Steering for Detailed Topic Extraction from Academic Literature",
        authors: "Takahiro Kawamura, Sho Enomoto and Jun-Ichiro Mori",
        abstract: "This study introduces domain-specific Small Language Models (SLMs) designed to fact-check the outputs of general-purpose Large Language Models (LLMs). The goal is to accurately and automatically extract detailed technical elements from academic papers to build knowledge graphs. Two SLM types were developed: one through continued pre-training and the other using attention steering. Experiments on information extraction in ‘Fake News Detection’ showed that SLMs improved the sufficiency, accuracy, and stability of extracted information. Future work will involve larger datasets and further knowledge graph construction."
    },
    {
        id: 'P2',
        title: "Avoiding Overpersonalization with Rule-Guided Knowledge Graph Adaptation for LLM Recommendations",
        authors: "Fernando Spadea and Oshani Seneviratne",
        abstract: "We present a lightweight neuro-symbolic framework to mitigate over-personalization in LLM-based recommender systems by adapting user-side Knowledge Graphs (KGs) at inference time. Instead of retraining models or relying on opaque heuristics, our method restructures a user’s Personalized Knowledge Graph (PKG) to suppress feature co-occurrence patterns that reinforce Personalized Information Environments (PIEs), i.e., algorithmically induced filter bubbles that constrain content diversity. These adapted PKGs are used to construct structured prompts that steer the language model toward more diverse, Out-PIE recommendations while preserving topical relevance. We introduce a family of symbolic adaptation strategies, including soft reweighting, hard inversion, and targeted removal of biased triples, and a client-side learning algorithm that optimizes their application per user. Experiments on a recipe recommendation benchmark show that personalized PKG adaptations significantly increase content novelty while maintaining recommendation quality, outperforming global adaptation and naive prompt-based methods."
    },
    {
        id: 'P3',
        title: "From Strings to Semantics: A Graph-based Reranking Approach for Annotating Tables using Domain Ontologies",
        authors: "Nan Liu, Mohamed-Anis Koubaa, Wolfgang Suess and Veit Hagenmeyer",
        abstract: "As one of the most widely used data storage and exchange formats, tabular data can be challenging to be integrated, interpreted, and reused when they lacks accurate semantic annotations, particularly when data come from heterogeneous sources. However, the annotation process is often time-consuming and requires a deep understanding of the internal structure of the target ontology. Therefore, developing efficient and accurate semi-automatic or fully automatic annotation tools is very important. Most existing approaches often rely on textual similarity to match column headers to ontology terms, and fail to effectively leverage the rich relational semantics representation within the ontology. To address this issue, we propose a reranking approach that combines semantic similarity with ontology structure. Specifically, we first generate a set of candidate ontology terms based on semantic similarity. For each source table header and its candidate ontology terms, we construct subgraphs and train a lightweight Graph Neural Network (GNN) model on these graphs to learn structure-aware representations. These representations are then used to improve the ranking of candidate ontology terms. To validate our approach, we performe experiments on the OAEI dataset. The results demonstrate that our approach improves the Hit@1 by 4% compared to a baseline model that only relies on lexical similarity. This result shows that learning on local subgraphs is a promising direction for ontology alignment and schema matching."
    },
    {
        id: 'P4',
        title: "Systematic Dataset Review: \\\\from Linked Dataset Discoverability to their FAIRness",
        authors: "Sana Latif and Maria Angela Pellegrino",
        abstract: "This work presents the \\texttt{Systematic Dataset Review} (shortened as SDR), a systematic, cyclic, and FAIR-aligned process for dataset discovery, curation, and quality assessment, inspired by Systematic Literature Review methodologies. The process consists of three phases: (i) a (multivocal) literature review to identify and document references about datasets; (ii) a Linked Dataset Discoverability phase to curate, filter, and publish datasets as domain-specific Linked Open Data sub-clouds; and (iii) a Quality Assessment phase that enables FAIRness evaluations through automated tools. The framework is adaptable across different use cases: monitoring the quality of existing sub-clouds, enriching current sub-clouds with new datasets, or constructing entirely new thematic sub-clouds. By ensuring transparency, reproducibility, and ongoing quality monitoring, this approach supports the creation of accessible and sustainable dataset ecosystems."
    },
    {
        id: 'P5',
        title: "Towards Actionable Ishikawa Diagrams: An Exploratory Case Study From the Textile Industry",
        authors: "Christian Fleiner, Simon Vandevelde and Joost Vennekens",
        abstract: "The Ishikawa diagram is an established tool in the manufacturing domain for conducting a root cause analysis. The Ishikawa diagram ontology was developed to explicitly model Ishikawa diagrams as visual artifacts, their encoded knowledge and the process of their creation. While formalization is an important step for making encoded knowledge accessible, another challenge is to establish reasoning mechanisms to provide decision-support to users. In this paper, we present a reasoning pipeline which resulted from a case study concerning fabric fault detection. The reasoning pipeline is intended to be used in conjunction with the Ishikawa diagram ontology."
    },
    {
        id: 'P6',
        title: "Using LLM to improve Knowledge Graph Entity Matching",
        authors: "Victor Eiti Yamamoto and Hideaki Takeda",
        abstract: "Knowledge graphs (KGs) are powerful tools for representing and reasoning over structured information. Entity matching between KGs helps integrate multiple KGs. However, the performance of entity matching tools can be sensitive to parameter settings, such as thresholds. Large language models (LLMs) have emerged as powerful tools for solving reasoning problems and show potential for improving entity alignment. Our approach incorporates two LLM-based steps: filtering and expansion. In the filtering step, an LLM is used to validate entity mappings. The expansion step then uses an LLM to select the correct mapping from a candidate list for any source entity that lacks a corresponding pair after the filtering step. Experiments on the OAEI KG track dataset and matchings with DBpedia datasets show that using an LLM as a filter achieves a low false-negative rate and a favorable false-positive rate, indicating that it can improve precision without significantly lowering recall. However, the expansion step has low precision because the LLM tries to select a corresponding entity even when no correct match exists in the candidate list."
    },
    {
        id: 'P7',
        title: "CE-KG: Citation Enhanced Knowledge Graph via Citation Sentiment Fusion and Evidence Tracing",
        authors: "Yalan Huang, Xuemei Yang, Bin Zhang and Xiaoli Tang",
        abstract: "This study proposes a knowledge graph (KG) construction method integrating citation information to address credibility tracing of knowledge claims. By fi-ne-tuning large language models (LLMs), SPO triples are precisely extracted from the \"Conclusion\" sections of literature abstracts. A dual-level fusion mechanism is developed based on sentiment analysis (Posi-tive/Neutral/Negative) of citation contexts. This approach injects academic evaluation attributes at the paper level and links them to SPO triples. An inno-vative Neo4j-MySQL heterogeneous storage architecture is designed, enabling fine-grained evidence tracing from KG relationships to citation contexts through uniform identifiers. The constructed KG simultaneously provides knowledge claims and supports credibility tracing from the academic communi-ty perspective."
    },
    {
        id: 'P8',
        title: "KGSynX: Knowledge Graph and Explainable Feedback Guided LLMs for Synthetic Tabular Data Generation",
        authors: "Ke Yu, Teruaki Hayashi, Yuki Shigoku, Yukari Usukura and Shigeru Ishikura",
        abstract: "Synthetic tabular data is vital for augmentation, privacy, and performance under limited data, yet most work targets marginal statistics, neglecting downstream utility and explainability in scarce-data scenarios. We propose KGSynX, which builds a knowledge graph from table records and derives graph embeddings to inform LLM prompts. A SHAP‑guided feedback loop measures attribution differences between real and generated data and injects targeted corrections into subsequent prompts. Evaluated under the Train-on-Synthetic, Test-on-Real (TSTR) protocol on heart disease, enterprise invoice, and telco churn datasets, KGSynX consistently outperforms baseline in accuracy, F1, and AUC while closing the SHAP attribution gap. By explicitly modeling structure and semantics, KGSynX produces more reliable synthetic datasets for downstream tasks."
    },
    {
        id: 'P9',
        title: "Constructing Cybersecurity Knowledge Graphs for Hybrid LLM–Graph Reasoning on Vulnerabilities",
        authors: "Julio Vizcarra, Yuta Gempei, Yanan Wang, Takamasa Isohara and Mori Kurokawa",
        abstract: "In cybersecurity, the threat landscape is composed of complex relations among security data and constantly evolves. To address this challenge, this paper presents a framework for constructing and reasoning over cybersecurity knowledge graphs (KGs) derived from vulnerability reports. Our approach analyzes textual content and structured data sources. To enhance causal reasoning, we explicitly model key causal factors as structured entities and relationships. The resulting KG is further enriched through augmentation using DBpedia, integrating external knowledge to enhance connectivity and context. We evaluate the impact of this augmentation through a comparison, contrasting the content of the original and the augmented graphs. Experimental results demonstrate that the Graph-LLM approach, with augmentation, enhances link prediction and produces higher-quality QA compared to using report descriptions alone. We demonstrate a hybrid reasoning setup integrating LLM-based language understanding with graph inference to answer cybersecurity queries."
    },
    {
        id: 'P10',
        title: "A Semantic Web-Based Infrastructure for Purpose-Driven Retrieval of Life Science Bioresources",
        authors: "Tatsuya Kushida, Daiki Usuda, Masanobu Yamagata, Norio Kobayashi, Shoichiro Shindo, Tatsuya Yamada, Yuki Yamagata and Hiroshi Masuya",
        abstract: "In the life sciences, the shared use of research materials for experiments is essential for ensuring reproducibility. Such materials are referred to as biological resources. To support life science research, biological resource centers are operated worldwide as institutional platforms for the provision of these materials. One of the core functions of these centers is disseminating information about available resources. The RIKEN BioResource Research Center, one of the bioresource center in Japan, has been providing a knowledge-based search system for life scientists since 2018. This system employs Semantic Web technologies to deliver detailed biological characteristics of the resources it manages. By integrating bioresource data, public life science datasets, and ontologies through a SPARQL endpoint backend, the system enables researchers to explore available research materials from diverse scientific perspectives via a dedicated search interface. Furthermore, the use of Semantic Web technologies supports sustainable and scalable system operation."
    },
    {
        id: 'P11',
        title: "Tackling the Write-to-Read Web of Data with Trustflows",
        authors: "Ben De Meester, Julian Andrés Rojas, Femke Ongenae, Pieter Colpaert and Ruben Verborgh",
        abstract: "The need for the read–write Linked Data Web is currently implemented as a Create, Read, Update or Delete (CRUD) strategy on resources that implies that agents will read exactly what has once been written. In practice, we notice the following problems when deploying writeable Linked Data nodes in an ecosystem. Data reuse is impeded by diverging data usage requirements from how the data was originally written and by evolving data models. Also, individual data point verification at the read side becomes obligatory due to the absence of an authoritative source that guarantees auditability. We present characteristics of writeable Linked Data nodes (cope with longevity for written data, combine read data using different models, and provide an explicit trust context) as arguments justifying the need for a more complex operational model. We apply a Command Query Responsibility Segregation pattern to decouple write and read interfaces, introduce semantic mappings within the writeable Linked Data node to support evolving data read requirements, and apply Event Sourcing aligned with PROV-O’s Actor-Entity-Activity model to provide an explicit trust context. Applying this operational model – dubbed “Trustflows” – to the PACSOI use case showcases the additional affordances of handling data coming in different granularities, from different sources, adhering to different reading requirements."
    },
    {
        id: 'P12',
        title: "Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction",
        authors: "Tsuyoshi Iwata, Guillaume Comte, Melissa Flores, Ryoma Kondo and Ryohei Hisano",
        abstract: "The growing importance of environmental, social, and governance data in regulatory and investment contexts has increased the need for accurate, interpretable, and internationally aligned representations of non-financial risks, particularly those reported in unstructured news sources. However, aligning such controversy-related data with principle-based normative frameworks, such as the United Nations Global Compact or Sustainable Development Goals, presents significant challenges. These frameworks are typically expressed in abstract language, lack standardized taxonomies, and differ from the proprietary classification systems used by commercial data providers. In this paper, we present a semi-automatic method for constructing structured knowledge representations of environmental, social, and governance events reported in the news. Our approach uses lightweight ontology design, formal pattern modeling, and large language models to convert normative principles into reusable templates expressed in the Resource Description Framework. These templates are used to extract relevant information from news content and populate a structured knowledge graph that links reported incidents to specific framework principles. The result is a scalable and transparent framework for identifying and interpreting non-compliance with international sustainability guidelines."
    },
    {
        id: 'P13',
        title: "Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets",
        authors: "Piotr Sowiński, Kacper Grzymkowski and Anastasiya Danilenka",
        abstract: "Recording data changes in RDF systems is a crucial capability, needed to support auditing, incremental backups, database replication, and event-driven workflows. In large-scale and low-latency RDF applications, the high volume and frequency of updates can cause performance bottlenecks in the serialization and transmission of changes. To alleviate this, we propose Jelly-Patch – a high-performance, compressed binary serialization format for changes in RDF datasets. To evaluate its performance, we benchmark Jelly-Patch against existing RDF Patch formats, using two datasets representing different use cases (change data capture and IoT streams). Jelly-Patch is shown to achieve 3.5–8.9x better compression, and up to 2.5x and 4.6x higher throughput in serialization and parsing, respectively. These significant advancements in throughput and compression are expected to improve the performance of large-scale and low-latency RDF systems."
    },
    {
        id: 'P14',
        title: "Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings",
        authors: "Hamid Ahmad, Heiko Paulheim and Rita T. Sousa",
        abstract: "Knowledge graphs and ontologies represent entities and their relationships in a structured way, having gained significance in the development of modern AI applications. Integrating these semantic resources with machine learning models often relies on knowledge graph embedding models to transform graph data into numerical representations. Therefore, pre-trained models for popular knowledge graphs and ontologies are increasingly valuable, as they spare the need to retrain models for different tasks using the same data, thereby helping to democratize AI development and enabling sustainable computing.  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API, designed to generate and serve knowledge graph embeddings for widely used biomedical ontologies. Given the dynamic nature of these ontologies, Bio-KGvec2go also supports regular updates aligned with ontology version releases. By offering up-to-date embeddings with minimal computational effort required from users, Bio-KGvec2go facilitates efficient and timely biomedical research."
    },
    {
        id: 'P15',
        title: "Constraint-Aware Ontology Engineering for Information Extraction from Financial Contracts",
        authors: "Ryoma Kondo, Shamik Kundu, Muneaki Imai, Kyosuke Tomoda, Yuki Takazawa and Ryohei Hisano",
        abstract: "The automation of business processes is advancing rapidly, particularly in domains governed by complex rule-based documentation such as financial contracts. These documents are highly structured and semantically rich, which makes them well suited for modeling with formal ontologies. Although large language models offer promising capabilities for information extraction, their effectiveness is limited by the challenge of designing consistent prompts across contract types due to a lack of standardized semantic definitions. In this paper, we explore how embedding ontological structures and constraint specifications into prompts can improve the accuracy and reusability of information extraction systems, using confirmation notices for investment transactions as a case study. Our findings show that incorporating semantic constraints into prompts improves performance in language model-based information extraction, highlighting the potential of combining Semantic Web technologies with language models to support accurate and maintainable information extraction from financial contracts."
    },
    {
        id: 'P16',
        title: "Prompt Compression for Dialog Navigation with Need-Oriented Environmental Knowledge Base and Large Language Models",
        authors: "Hiroaki Shimoma, Sudesna Chakraborty and Takeshi Morita",
        abstract: "In Embodied AI, navigation agents using Large Language Models (LLMs) often rely on lengthy prompts that include extensive environmental information. This becomes increasingly problematic in complex environments such as the VirtualHome simulator, where incorporating all object data can reduce accuracy and increase computational costs. To address this, we propose a prompt compression technique based on a ``need-oriented'' environmental knowledge base. Our system first infers a user's underlying need from their natural language request using Murray's theory of human needs. It then retrieves only the objects relevant to that need from our knowledge base. A compressed prompt, containing only the user's request and the specific objects, is then sent to the LLM. The results showed this method significantly improves navigation accuracy while reducing prompt length compared to approaches that use all environmental data."
    },
    {
        id: 'P17',
        title: "Interpreting User Needs with LLMs-based Conversational Agents and Knowledge Graphs: An Earth Observation Use Case",
        authors: "Antoine Dupuy, Nathalie Aussenac-Gilles, Christophe Baehr and Cassia Trojahn",
        abstract: "Open Science has broadened access to scientific datasets. However, identifying relevant ones to specific user needs remains challenging due to the volume, diversity, and poor metadata. This paper proposes to integrate semantically enriched metadata with LLM agents to interpret natural language queries, to extract user intent, and to generate justifications for retrieved results. Experiments with different LLMs highlight the potential of such approach for scientific dataset retrieval."
    },
    {
        id: 'P18',
        title: "TyRaL: End-to-End Document-level Relation Extraction via Type-Constrained Rule Learning",
        authors: "Mierzhati Alimu, Xiaowang Zhang and Chaochao Du",
        abstract: "In recent years, Document-level Relation Extraction (DocRE) has encountered significant challenges in capturing complex entity relationships and reasoning over long-range dependencies. Existing methods primarily focus on learning implicit representations or applying chain-like logical rules, but they often overlook differences in entity types and the significance of type constraints, potentially leading to errors in relation reasoning. This poster introduces a type-constrained enhanced chain-like rule (TC rule) and proposes an end-to-end document-level relation extraction framework (TyRaL) to address this issue. By incorporating a novel rule reasoning module, TyRaL transforms the discrete rule learning problem into a parameter optimization task in continuous space, enabling both explicit and implicit learning of entity type constraint rules and thereby enhancing the model's logical consistency and interpretability. Experimental results on the standard DWIE dataset show that TyRaL significantly outperforms existing rule-enhanced methods in both F1 and Ign F1 metrics. It demonstrates superior logical modeling and semantic reasoning capabilities while offering new perspectives and solutions for research in the DocRE field."
    },
    {
        id: 'P19',
        title: "Human-Friendly Explanation for Ontology-based Concept Similarity: Design and Development",
        authors: "Watanee Jearanaiwongkul and Teeradaj Racharak",
        abstract: "While recent neuro-symbolic approaches have enabled interpretable computation of concept similarity in ontologies, translating these formal explanations into human-friendly forms remains a challenge. In this work, we investigate how large language models, particularly ChatGPT and Gemini, can be prompted to generate natural language explanations that justify similarity results in a way that is understandable to end users. Building on a neuro-symbolic framework for measuring concept similarity in Description Logic (DL) ontologies, we explore two types of human-friendly explanations i.e., node-based and path-based explanation. Furthermore, we evaluate LLMs's ability to generate each component of these path-based explanations using our small curated dataset. We evaluate the effectiveness of prompting approaches along different dimensions such as clarity, informativeness, and perceived usefulness through both qualitative analysis and user studies. Our results show the potential and limitations of using LLMs as a tool to bridge the gap between formal similarity reasoning and human interpretability, paving the way for more transparent ontology-driven systems."
    },
    {
        id: 'P20',
        title: "JSimELHExplainer: A Robust JAVA Library for Explainable Semantic Similarity for ELH Description Logic Ontology",
        authors: "Teeradaj Racharak and Watanee Jearanaiwongkul",
        abstract: "We present a newly developed Java library that implements a neuro-symbolic framework for computing semantic similarity between concepts in Description Logic ELH ontologies. This library provides an implementation of a hybrid approach combining a structural-based method in ontology reasoning with distributional semantics derived from pre-trained word embeddings. It supports efficient similarity computation and interpretable explanations for the results. Designed with scalability in mind, it guarantees polynomial-time execution and supports large-scale ontologies with thousands of concepts and complex hierarchical structures. For explainability, it produces fine-grained explanations by identifying the contributing primitive and existential concept pairs, as well as the semantic alignments found in the embedding space. These explanations help users understand why a similarity score is given, making the results transparent and auditable. The API is embedding-agnostic and compatible with a wide range of vector space models, including static embeddings (e.g., Word2Vec) and contextualized models (e.g., BERT). This tool enables the development of explainable, knowledge-driven AI systems in domains where both structured ontological modeling and contextual semantic understanding are essential."
    },
    {
        id: 'P21',
        title: "Learned Indexing for Efficient Querying on Knowledge Graphs",
        authors: "Gaurav Dawra, Aanchal Gupta, Nandika Jain, Yogender Kumar, Jishnu Raj Parashar, Bapi Chatterjee and Raghava Mutharaju",
        abstract: "Knowledge Graphs are used extensively in several domains across many different applications. They are good at integrating, managing, and extracting value from diverse sources of data. Often, they are very large, with billions of nodes and edges. SPARQL, a W3C standard, is one of the prominent query languages for Knowledge Graphs. Even simple SPARQL queries typically involve several joins. Naturally, such queries applied to large Knowledge Graphs have huge latency. There have been several works on optimizing and finetuning SPARQL queries. However, they do not consider the distribution of the underlying data. Learned indexes provide queries based on data distribution with improved throughput and reduced space. In this work, we introduce a learned index for implementing SPARQL queries in Knowledge Graphs. Our evaluation on two Knowledge Graphs involving a few SPARQL queries led to promising results that encourage further research in this direction."
    },
    {
        id: 'P22',
        title: "SSHOC-NL: Towards a Knowledge Graph for Social Sciences and Humanities",
        authors: "Andre Valdestilhas, Ronald Siebes and Jacco van Ossenbruggen",
        abstract: "Social Sciences and (Digital) Humanities (SSH) increasingly benefit from the online availability of datasets, services, and tools provided by their peers and governmental organizations. However, the fragmented and heterogeneous nature of the metadata makes it difficult to assess the complexity of replicating results and the potential reuse of various parts (methodology, tools and data) for follow-up research. For example, a significant effort is required to check the access requirements and quality of the data and to check if the format is compatible with tools that a researcher is familiar with. This paper outlines the vision, structure, and objectives of the Social Science and Humanities Open Cloud for the Netherlands (SSHOC-NL), focusing on its role in building and leveraging a national SSH knowledge graph. By developing a unified data environment, semantic integration tools, and interactive Data Stories, this SSHOC-NL effort transforms disparate data into interconnected scientific knowledge. This work aims to equip Dutch researchers with the tools and skills necessary to navigate complex societal challenges through a rich, semantically linked data landscape, representing a critical step towards realizing the promise of the European Open Science Cloud (EOSC) within the Dutch SSH domain."
    },
    {
        id: 'P23',
        title: "Exploring LLM to extract Knowledge Graph from academic abstracts",
        authors: "Victor Eiti Yamamoto, Othmane Kabal, Lakshan Karunathilake, Kotaro Nishigori, Vicente Lermanda, Shixiong Zhao, Hiroki Uematsu, Yanming He and Hideaki Takeda",
        abstract: "Knowledge graphs (KGs) are a powerful tool for representing semantic information. Existing methods depend on the use of human annotation or semi-structured automated methods. However, academic papers and their abstracts are still the main way to carry academic information. The development of LLM leads to new tools to solve semantically heavy problems, so LLM can help to create KGs from texts automatically. In this research, we tested three approaches on three abstracts from computer science. We found that all methods still have room to improve precision and recall. Furthermore, finding a common way to express the same sentence in a triple is one of the significant issues in comparing abstracts"
    },
    {
        id: 'P24',
        title: "Assessing Logical Inference Capabilities of Large Language Models through RDF Schema Entailment Rules: A Multi-Level Evaluation",
        authors: "Taichi Hosokawa, Sudesna Chakraborty and Takeshi Morita",
        abstract: "Large language models (LLMs) achieve strong performance in various language tasks, yet their logical inference abilities remain limited. LLMs often rely on pre-trained knowledge rather than explicit inference. Their inference capabilities in ontology languages like RDFS also remain underexplored. This study evaluates LLMs' inference abilities using RDFS entailment rules with two knowledge datasets: real-world data from Linked Open Data and counterfactual data created by systematically altering real-world facts. We propose a novel evaluation methodology assessing LLM outputs. To analyze inference behavior under different conditions, we design a three-level task framework varying rule presentation methods for identical inference tasks. Results show high accuracy on real-world datasets. LLMs sometimes infer missing premises using pre-trained knowledge, suggesting potential for incompletely structured environments. However, accuracy declines with counterfactual datasets and when shifting from pre-combined to multiple separate rules. Performance further drops when models must select appropriate rules from predefined subsets. These findings highlight both strengths and limitations of LLMs in structured, rule-based inference within ontology-driven systems."
    },
    {
        id: 'P25',
        title: "Key Aspect Prediction for Silent Vulnerability Fixes via Semantic Augmentation",
        authors: "Dongshun He, Linyi Han and Xiaowang Zhang",
        abstract: "Silent vulnerability fixes pose significant risks to downstream open-source software (OSS) users, as the lack of vulnerability details in fix patches leaves users unaware of potential threats. Previous work predicts the key aspects of vulnerability fixes using an encoder-decoder model to aid users in understanding these fixes. However, their approach overlooks the limited expressiveness of commit messages and the varied intents underlying code changes. In this poster, we propose a semantic-augmented method for key aspect prediction in silent vulnerability fixes. Firstly, we enrich commit semantics by incorporating information from multiple external sources. Then, we design a Chain-of-Thought (CoT) prompt to analyze code semantics at the hunk level and identify security-relevant changes. Finally, we design a task-specific embedding method to represent code diffs and retrieve semantically similar commits, guiding large language models (LLMs) to predict the vulnerability type, root cause, impact, and attack vector. Experiments on our constructed dataset demonstrate that our method outperforms baselines in key aspect prediction across ROUGE-L and METEOR."
    },
    {
        id: 'P26',
        title: "MetaExplainer In Action: An Overview of a Framework to Generate Multi-Type User-Centered Explanations",
        authors: "Shruthi Chari, Oshani Seneviratne, Prithwish Chakraborty, Pablo Meyer and Deborah McGuinness",
        abstract: "Explanations are crucial for building trustworthy AI systems, but a gap often exists between the explanations provided by models and those needed by users. Previous research and our interactions with clinicians have shown that users prefer question-driven and diverse explanations. To address this gap, we introduce MetaExplainer, a neuro-symbolic framework designed to generate user-centered, multi-type explanations. Our approach employs a three-stage process: first, we decompose user questions into machine-readable formats using state-of-the-art large language models (LLM); second, we delegate the task of generating system recommendations to model explainer methods; and finally, we synthesize natural language explanations that summarize the explainer outputs. Here, we present an indicative end-end example from the Diabetes (PIMA Indian) tabular dataset. We describe the MetaExplainer's implementation and quantitative and qualitative results of the framework in a separate, longer paper that we link here. Overall, the MetaExplainer is a versatile and traceable neuro-symbolic explanation generation framework addressing a broad range of user questions, positioning MetaExplainer as a promising tool for enhancing AI explainability across multiple domains. Github: \\href{https://github.com/tetherless-world/metaexplainer}{https://github.com/tetherless-world/metaexplainer}."
    },
    {
        id: 'P27',
        title: "Ontologies, Knowledge Graphs, and LLMs: How Do We GET Evaluations Done Right?",
        authors: "Heiko Paulheim",
        abstract: "The use of Large Language Models (LLMs) becomes increasingly popular for many tasks in the Semantic Web and Knowledge Graph community, e.g., knowledge graph (KG) construction, ontology learning, and ontology matching. Methods and tools using LLMs for those tasks are often evaluated on existing KGs and ontologies, which are publicly available on the Web. Thus, it can be assumed that the test data has been seen by the LLM, and it is questionable if the results transfer to a case of unseen data (which is where those models are intended to be employed).  In this paper, we question the current evaluation paradigm using public data and propose a different approach, i.e., using a secondary LLM to create ontologies and knowledge graphs for one-time use on the fly. We coin this approach GET (generate--evaluate--trash). This also allows for repeating experiments and computing standard deviations and confidence intervals, which facilitates additional statements about the robustness of different approaches. We demonstrate our suggested approach on the case of taxonomy induction."
    },
    {
        id: 'P28',
        title: "GraphRAG with Knowledge Graphs for Question Answering on Administrative Meeting Records",
        authors: "Kumi Ushio, Daichi Tsuji and Yohei Kobashi",
        abstract: "This study introduces a GraphRAG-based question-answering system for Japanese administrative meeting records, addressing challenges in accessing policy-related information. Using the minutes from Japan’s Financial Services Agency, we constructed a lightweight ontology-aware knowledge graph capturing participants, meetings, and utterances, and integrated it with LLMs. The system applies a GraphRAG approach, leveraging graph-based context expansion to integrate related nodes and enrich contextual understanding, combined with dynamic tool selection to support multi-step reasoning. Evaluation with questions showed high accuracy for both simple retrieval and relation-exploration queries. Future work includes improving retrieval accuracy, developing domain-specific ontologies, automating tool generation, and deploying the system as an interactive application."
    },
    {
        id: 'P29',
        title: "Accelerating Drug Discovery through Semantic Data Integration and Machine Learning: From Biomedical Knowledge Graphs to Predictive Models",
        authors: "Toshiaki Katayama, Shuichi Kawashima, Ryosuke Kojima, Takuto Koyama, Mayumi Kamada and Yuki Moriya",
        abstract: "We introduce two interoperable resources that facilitate semantic data integration and machine learning in biomedical research: the med2rdf knowledge graph and the Tabulae dataset preparation system. med2rdf transforms heterogeneous biomedical databases into RDF using a unified ontology and publishes them via the RDF Portal with SPARQL endpoints and FAIR-compliant metadata. It addresses key integration issues such as identifier heterogeneity and vocabulary inconsistency. Tabulae builds on this semantic infrastructure by enabling users to generate machine learning–ready tabular datasets from RDF-based sources. It abstracts complex querying and integrates compound and protein features into a unified format. We demonstrate the utility of these resources through a case study on compound–protein interaction (CPI) prediction using Random Forest regression. This illustrates how semantic integration combined with machine learning can support efficient drug discovery. Together, med2rdf and Tabulae provide a scalable and reusable framework for semantic data-driven research in biomedicine."
    },
    {
        id: 'P30',
        title: "An Ontology for the Common Data Format on Football Match Data",
        authors: "Fajar J. Ekaputra, Gregor Käfer and Matthias Kempe",
        abstract: "Applications of artificial intelligence (AI) in sports, particularly for football (soccer) has been growing in the last years, e.g., for player recruitments, performance monitoring, as well as player selection. To support such applications, the availability of an integrated, high-quality data is crucial to ensure accurate results. This aspect is especially crucial due to the heterogeneity in data acquired by various stakeholders, e.g., companies and football clubs. Catering for such demand, a recent work proposed a minimal schema for football match data called the common data format (CDF), aiming to ensure the provided data is clear, sufficiently contextualized, and complete to enable common downstream analysis tasks. This paper reports on an initial effort to create the Football Common Data Format (FCDF) ontology as a serialization of the CDF core concepts, with a particular focus on streamlining concepts, properties, and attributes. The FCDF ontology aims to provide a formal, shared conceptualization of CDF as a symbolic model, to facilitate further development of AI in sports, particularly through neurosymbolic AI approaches."
    },
    {
        id: 'P31',
        title: "Unveiling the Butterfly Effect in Knowledge Editing for Large Language Models Using Knowledge Graph-based Analysis",
        authors: "Patipon Wiangnak, Natthawut Kertkeidkachorn and Kiyoaki Shirai",
        abstract: "Large Language Models (LLMs), particularly those based on Generative Pre-trained Transformers (GPT), have achieved strong performance in various natural language tasks. However, LLMs are limited by a knowledge cut-off, so their information is not updated. Common methods for updating LLM knowledge, such as fine-tuning, retrieval-augmented generation, and machine unlearning, are often resource-intensive and may introduce unintended effects, including the loss of relevant context or conflicts with existing knowledge. Knowledge Editing (KE) offers a more efficient alternative by enabling precise updates to specific facts without retraining the entire model, while preserving unrelated information. Still, such edits can trigger unexpected ripple effects, known as the Butterfly Effect, where modifying one fact causes errors in related knowledge. In this work, we introduce ButterflyKE, a knowledge graph-based analysis method that probes neighboring knowledge to identify local side effects caused by a single factual update. Using Wikidata as a reference knowledge graph, in ButterflyKE, we extract directly connected triples to provide a structural view of how knowledge propagates after editing. We evaluate three main KE approaches: External Memory-based, Global Optimization-based, and Local Modification-based approaches, using the Llama-3.1-8B-Instruct model. Our findings confirm the presence of the Butterfly Effect in KE, with side effects intensifying as the structural connections increase. To measure this impact, we propose the Butterfly Index, a metric to evaluate editing methods and their influence on surrounding knowledge. ButterflyKE serves as a practical method for extending existing benchmarks and supports a deeper analysis of knowledge integrity in LLM."
    },
    {
        id: 'P32',
        title: "From Culture to Core: Integrating Cultural Heritage Data into Cross-Domain Research Infrastructures",
        authors: "Tabea Tietz, Linnaea Söhn, Oleksandra Bruns, Joerg Waitelonis, Etienne Posthumus, Jonatan Jalle Steller, Torsten Schrade and Harald Sack",
        abstract: "Within the National Research Data Infrastructure (NFDI) in Germany, the NFDI4Culture consortium addresses a critical challenge: unifying access to fragmented and semantically heterogeneous cultural heritage (CH) research data scattered across institutions and disciplines. This work presents the NFDI4Culture Ontology (CTO), a strategically designed lightweight, BFO-aligned ontology that successfully bridges the gap to represent CH research resources between specialized domain requirements and interoperability demands across diverse cultural heritage fields including musicology, performing arts, and architecture. CTO extends the established mid-level ontology NFDIcore while maintaining the flexibility essential for capturing domain-specific nuances and is fully integrated into productive research data infrastructures. This contribution demonstrates how domain-specific ontologies can support both highly specialized research needs and broader cross-domain interoperability through modular architecture, and provides insights into proven modeling strategies, integration workflows, and lessons learned from a productive system in the CH domain."
    },
    {
        id: 'P33',
        title: "Comparing Methods for Competency Question Elicitation from Ontology Requirements",
        authors: "Reham Alharbi, Jacopo de Berardinis, Terry Payne and Valentina Tamma",
        abstract: "Competency Questions (CQs) are used guide ontology development, yet formulating them in such a was as to align them to the stakeholder needs remains challenging. This paper presents a comparative analysis of three CQ elicitation methods: manual authoring by ontology engineers, template-based instantiation, and automated generation using LLMs (GPT-4.1, Gemini 2.5). Each CQ is evaluated across dimensions of suitability, readability, and complexity. To facilitate this evaluation we introduce AskCQ, a dataset of 204 CQs derived from a shared user story in the cultural heritage domain. Our results show that manually authored CQs are consistently more acceptable, readable, and concise. LLM-generated CQs are more complex and diverse but require refinement. These findings highlight the importance of human expertise and suggest potential hybrid approaches."
    },
    {
        id: 'P34',
        title: "CogNet3: Fusing Dynamic Emotional Knowledge of Personality Homophilous Groups in Real-World Events into Multi-Source Knowledge Graph",
        authors: "Tong Zhou, Yubo Chen, Kang Liu and Jun Zhao",
        abstract: "In this paper, we present CogNet3, an extension of the CogNet2 knowledge base, which combines the dynamic emotional knowledge of personality groups towards significant events from real word data on Reddit. It aims to structurally model and correlate the subjective emotional knowledge embedded in events. To model the dynamic and complex multi-dimensional emotional information of different types of people towards complex events, we construct three frames, namely Semantic Event, Homophilous Group, and Group Emotion, which are respectively used to model hierarchical organizational events with emotional information, user groups with representative differences in personality attributes, and the multi-dimensional dynamic emotional distribution between user groups and events. To expand the knowledge scale and enhance scalability, we design a LLM information extraction framework with self-verification capabilities for the automated extraction of subjective knowledge information. As a result, in comparison with CogNet2, CogNet3 increases 462,381 new event instance with emotion association, 21,870 different homophilous groups and up to 4,556,057 emotion distribution instances."
    }
];

export { posters };