const resourceTrack = [
    {
        title: "mmRAG: A Modular Benchmark for Retrieval-Augmented Generation over Text, Tables, and Knowledge Graphs",
        authors: "Chuan Xu, Qiaosheng Chen, Yutong Feng and Gong Cheng",
        abstract: "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the capabilities of large language models. However, existing RAG evaluation predominantly focuses on text retrieval and relies on opaque, end-to-end assessments of generated outputs. To address these limitations, we introduce mmRAG, a modular benchmark designed for evaluating multi-modal RAG systems. Our benchmark integrates queries from six diverse question-answering datasets spanning text, tables, and knowledge graphs, which we uniformly convert into retrievable documents. To enable direct, granular evaluation of individual RAG components---such as the accuracy of retrieval and query routing---beyond end-to-end generation quality, we follow standard information retrieval procedures to annotate document relevance and derive dataset relevance. We establish baseline performance by evaluating a wide range of RAG implementations on mmRAG."
    },
    {
        title: "RDFMutate: Mutation-Based Generation of Knowledge Graphs",
        authors: "Tobias John, Einar Broch Johnsen and Eduard Kamburjan",
        abstract: "This paper introduces RDFMutate, the first mutation-based tool to generate RDF knowledge graphs. RDFMutate enables developers to analyze the robustness of applications that operate on RDF, by gen- erating variants of seed knowledge graphs, which are mutated according to a set of mutation operations and SHACL constraints. In contrast to existing tools to generate synthetic RDF graphs, RDFMutate provides a mutation-based approach that is accessible for both researchers and ap- plication developers, by providing a framework to define mutation rules and flexible selection of generated graphs."
    },
    {
        title: "Sparqloscope: A generic benchmark for the comprehensive and concise performance evaluation of SPARQL engines",
        authors: "Hannah Bast, Johannes Kalmbach, Christoph Ullinger and Robin Textor-Falconi",
        abstract: "We provide a new benchmark, called Sparqloscope, for evaluating the query performance of SPARQL engines. The benchmark combines three unique features, which separates it from other such benchmarks: 1. Sparqloscope is generic in the sense that it can be applied to any given RDF dataset and it will then produce a comprehensive benchmark for that particular dataset. Existing benchmarks are either synthetic, designed for a fixed dataset, or require query logs. 2. Sparqloscope is comprehensive in that it considers most features of the SPARQL 1.1 query language that are relevant in practice. In particular, it also considers advanced features like EXISTS, various SPARQL functions for numerical values, strings, and dates, language filters, etc. 3. Sparqloscope is specific in the sense that it aims to evaluate relevant features in isolation and as concisely as possible. In particular, the benchmark generated for a given knowledge graph consists of only around 100 very carefully crafted queries, the results of which can and should be studied individually and not in aggregation. Sparqloscope is free and open-source software and easy to use. As a showcase we use it to evaluate the performance of three high-performing SPARQL engines (Virtuoso, MillenniumDB, QLever) on two widely used RDF datasets (DBLP and Wikidata).",
        spotlight: true
    },
    {
        title: "cnChemNER：A Dataset for Chinese Chemical Named Entity Recognition",
        authors: "Haoran Zhang, Tingxin Jiang, Hongxia Jin, Ying Yang, Xiaowang Zhang and Zhiyong Feng",
        abstract: "Due to the inherent challenges in dataset construction, no comprehensive Chinese Chemical Named Entity Recognition (CNER) dataset has been publicly released to date. As a result, there is a scarcity of domain-specific Chinese datasets in the field of chemistry, impeding progress in key intelligent chemistry tasks such as reaction mechanism analysis, automatic literature classification, and cross-lingual terminology alignment. This work addresses this gap by introducing the first large-scale Chinese CNER dataset—cnChemNER—which leverages granted Chinese patents as a data source to develop a rule-based, patent-oriented automatic extraction framework. The dataset comprises 76,245 annotated chemical entities spanning four primary categories, which are further divided into 73 fine-grained subcategories. The experimental results validate the effectiveness of using patents as a data source, with language models trained on cnChemNER demonstrating superior performance across precision, recall, and F1-score metrics. cnChemNER bridges a critical gap in Chinese chemical corpora and represents a significant advancement in terms of annotation granularity, semantic structure, and domain-specific adaptability. We anticipate that cnChemNER will serve as a valuable resource for enhancing model performance in cross-lingual chemical text processing. Additionally, it can support downstream tasks in related vertical domains such as biomedicine and materials science."
    },
    {
        title: "Virtual Knowledge Graphs over Earth Observation Data",
        authors: "Albulen Pano, Davide Lanti and Diego Calvanese",
        abstract: "Earth Observation (EO) data publication and dissemination continues to grow driven by the increase in satellite launches. openEO is one of the most well known platform to query EO data (in the form of datacubes) using web API. To be truly valuable, Earth Observation (EO) data often needs to be combined with other data sources, like relational databases. Knowledge graphs offer a way to bridge the semantic gap between EO data and these additional sources. However, the execution of integrated queries can run into scalability issues due to the enormous volume of EO data. In this paper, we propose addressing this challenge through the use of Virtual Knowledge Graphs---a paradigm that presents data to end-users as a knowledge graph, while keeping the data in its original sources rather than materializing it in graph form. We show the feasibility of our approach by implementing a prototype solution and test it over real world openEO examples."
    },
    {
        title: "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs",
        authors: "Antonis Klironomos, Baifan Zhou, Zhipeng Tan, Zhuoxun Zheng, Mohamed H. Gad-Elrab, Heiko Paulheim and Evgeny Kharlamov",
        abstract: "Nowadays machine learning (ML) practitioners have access to numerous ML libraries available online. Such libraries can be used to create ML pipelines that consist of a series of steps where each step may invoke up to several ML libraries that are used for various data-driven analytical tasks. Development of high-quality ML pipelines is non-trivial; it requires training, ML expertise, and careful development of each step. At the same time, domain experts in science and engineering may not possess such ML expertise and training while they are in pressing need of ML-based analytics. In this paper, we present our ExeKGLib, a Python library enhanced with a graphical interface layer that allows users with minimal ML knowledge to build ML pipelines. This is achieved by relying on knowledge graphs that encode ML knowledge in simple terms accessible to non-ML experts. ExeKGLib also allows improving the transparency and reusability of the built ML workflows and ensures that they are executable. We show the usability and usefulness of ExeKGLib by presenting real use cases.",
        spotlight: true,
    },
    {
        title: "HealthEQKG: A Knowledge Graph and Data Model for Health Equity Research",
        authors: "Navapat Nananukul and Mayank Kejriwal",
        abstract: "Owing to the expense of healthcare and aging demographics, health inequity has emerged as a critical challenge in the United States, leading to significant disparities in health outcomes and unequal access to care in different communities. Understanding the complex associations between the distribution of healthcare providers, clinician characteristics, and socioeconomic conditions in their community of practice is essential to developing policies to close health inequity gaps. However, research in this domain is challenging due to the fragmentation of relevant datasets (including by the government), and the difficulty of using these datasets in a semantically unified manner. To address this challenge, we introduce HealthEQKG, an open-source knowledge graph (KG) specifically designed to support both qualitative and quantitative health equity research. Supported by a compact underlying ontology, HealthEQKG integrates two national-level (but independent) government agency datasets containing physician data and socioeconomic data, followed by data augmentation through the use of established Semantic Web resources like DBpedia Spotlight. The complete KG contains 72,658 physicians and 28,346 Area Deprivation Indices for zip codes across the US. Through a series of fifteen competency questions, and two use-cases, we demonstrate its utility as a queryable public resource for public health policymakers and researchers."
    },
    {
        title: "A Domain Ontology for Ishikawa Diagrams to Enhance Root Cause Analysis",
        authors: "Christian Fleiner, Duo Yang, Simon Vandevelde and Joost Vennekens",
        abstract: "Ishikawa diagrams, also known as fishbone or cause-and-effect diagrams, are a widely known visual tool for performing root cause analysis (RCA). Although Ishikawa diagrams originated in the manufacturing sector, the tool is also actively used in other areas such as healthcare or business due to its simple structure, which requires little or no training beforehand. Though Ishikawa diagrams are valuable sources of knowledge, they lack rich semantics to effectively process them. As a result, knowledge engineers tend to ignore Ishikawa diagrams and choose other means to collect knowledge, although domain experts are familiar with the RCA tool and it is highly accepted. This paper presents the Ishikawa diagram ontology which enables the explicit modeling of Ishikawa diagrams as visual artifacts, their encoded knowledge and the process of their creation by reusing and extending existing ontologies. The ontology was developed using the LOT methodology. We have created a dataset of Ishikawa diagrams and describe a fictional use case to illustrate the intended use of the presented ontology."
    },
    {
        title: "Co-creating an Ontology of Online Gender-Based Harms: An Interdisciplinary Perspective",
        authors: "Miriam Fernandez, Alba Catalina Morales Tirado, Angel Pavon-Perez, Keely Duddin, Min Zhang, Ksenia Bakina, Arosha Bandara, Rose Capdevila, Lisa Lazard and Olga Jurasz",
        abstract: "The rising prevalence and complexity of online harms, particularly those disproportionately affecting women, demand urgent, interdisciplinary, and socio-technical responses. Despite increasing awareness and policy action, current responses remain fragmented across disciplinary silos, limiting the development of cohesive and effective interventions. This paper presents our efforts to co-create a comprehensive Gender-aware Ontology of Online Harms as a shared knowledge structure to bridge disciplinary perspectives and inform practice across sectors, including policing, law, behavioural science, and technology. Our ontology aims to capture the full spectrum of gender-based online harms, their sociotechnical enablers and inhibitors, and their manifestations in online contexts. The development process employs a co-creation approach grounded in collaborative ontology engineering and iterative stakeholder engagement. It has been driven by discipline-specific Personas and Competency Questions to ensure relevance, usability, and impact across diverse domains. We argue that this work represents a crucial step toward formalising a shared understanding of online harms to support policy reform, technological innovation, and survivor support."
    },
    {
        title: "COTTAS: Columnar Triple Table Storage for Efficient and Compressed RDF Management",
        authors: "Julián Arenas-Guerrero and Sebastián Ferrada",
        abstract: "One of the main challenges in working with RDF data is its verbosity, as repeated IRIs and IRI prefixes lead to large files that are costly to store and process. HDT, a binary RDF format, addresses this by compressing data while supporting efficient triple pattern evaluation without decompression. However, its performance is highly dependent on index alignment with query patterns. In this paper, we introduce COTTAS, a storage model that encodes RDF graphs directly into the open-source Apache Parquet columnar format. COTTAS represents RDF as a triple table and leverages block range indexes (zone maps) to achieve high compression ratios and fast query execution over compressed data. We also provide pycottas, an open-source Python library that enables compression of RDF data into COTTAS format and supports efficient querying by translating triple patterns into SQL queries over COTTAS files. This implementation facilitates the adoption of COTTAS for managing RDF graphs. Experiments on the WDBench and DBpedia benchmarks show that COTTAS reduces storage requirements around 50% with respect to HDT and exhibits competitive triple pattern evaluation, with less performance volatility across pattern types."
    },
    {
        title: "Benchmarking Knowledge Editing using Logical Rules",
        authors: "Tatiana Moteu Ngoli, N'Dah Jean Kouagou, Hamada Zahera and Axel-Cyrille Ngonga Ngomo",
        abstract: "The increasing use of large language models across domains reveals and effectuates a growing need for regularly updated knowledge in foundation models. However, retraining these models is often prohibitively expensive. Knowledge editing is hence becoming an increasingly popular means to maintain foundation models up-to-date or even correct erroneous assertions they propagate. However, it is still unclear how current knowledge editing approaches really perform, as current evaluation studies are mostly limited to validating the recall of edited facts with no regard for their logical consequences. To overcome this limitation, we present a new benchmark for knowledge editing that includes multi-hop questions generated using logical rules to evaluate the effectiveness of knowledge editing methods. Our findings reveal that while existing knowledge editing approaches can accurately insert assertions into large language models, they often fail to inject entailed knowledge. In particular, our experiments on the popular approaches ROME and FT suggest a considerable performance gap of up to 24% between evaluations on directly edited knowledge and on entailed knowledge, hence highlighting the need for semantics-aware evaluation frameworks for knowledge editing."
    },
    {
        title: "SSBD Ontology: A Two-Tier Approach for Interoperable Bioimaging Metadata",
        authors: "Yuki Yamagata, Koji Kyoda, Hiroya Itoga, Emi Fujisawa and Shuichi Onami",
        abstract: "Advanced bioimaging technologies have enabled large-scale capture of multidimensional data, yet effective metadata management and interoperability remain significant challenges. To address these needs, we propose a new ontology-driven framework for the Systems Science of Biological Dynamics Database (SSBD). We propose an ontology-driven framework for the Systems Science of Biological Dynamics Database (SSBD) that adopts a two-tier architecture. The core layer provides a class-centric structure referencing existing biomedical ontologies, supporting both SSBD:repository—focused on rapid dataset publication with minimal metadata—and SSBD:database—enhanced with biological and imaging-related annotations. Meanwhile, the instance layer houses actual imaging datasets as RDF instances. This layered approach aligns flexible instance-based repositories with robust ontological classes, enabling seamless integration and advanced semantic queries. By coupling flexibility with rigor, the SSBD Ontology promotes interoperability, data reuse, and the discovery of novel biological mechanisms. Moreover, our solution aligns with the REMBI guidelines and fosters compatibility. Ultimately, our approach contributes to establishing a FAIR data ecosystem across the bioimaging community."
    },
    {
        title: "From Legal Texts to Structured Knowledge: A Comprehensive Pipeline for Legal Text Summarization",
        authors: "Ahmad Sakor, Kuldeep Singh and Maria-Esther Vidal",
        abstract: "This paper introduces a comprehensive and generalizable pipeline for legal text summarization, with the low-resource Arabic language serving as a use case. Designed to bridge the gap between raw legal documents and structured knowledge representation, our approach leverages a dataset of 24,656 Moroccan legal cases, encompassing data extraction and processing, LLM-assisted annotations for gold-standard summaries, and a knowledge distillation approach to transfer summarization capabilities from a teacher model to a fine-tuned smaller LLM to produce structured, JSON-formatted summaries adhering to a specific template. Our fine-tuning approach achieves substantial performance improvement over the base LLM specifically on Moroccan legal text summarization and structured output generation, with evaluation metrics demonstrating an increase in summarization precision of up to 26\\%. Furthermore, we extend the application of this resource by constructing an interactive knowledge graph that visualizes relationships between cases, legal principles, and parties involved, enabling advanced queries and legal trend analysis. The publicly available dataset and fine-tuned model as resources enable reproducibility and offer a scalable solution for structuring legal knowledge in low-resource languages like Arabic."
    },
    {
        title: "SHACL Dashboard: Analyzing Data Quality Reports over Large-Scale Knowledge Graphs",
        authors: "Johannes Mäkelburg, Zenon Zacouris, Jin Ke and Maribel Acosta",
        abstract: "Validating knowledge graphs (KGs) ensures their quality and reliability in real-world applications. The Shapes Constraint Language (SHACL) has emerged as a recommended standard for validating RDF KGs, by defining structured constraints. Many organizations leverage SHACL validation and its reports to detect problems, guide corrections, and improve data quality. Yet, large-scale KGs often produce extensive validation reports, making manual analysis infeasible. To address this challenge, we present SHACL Dashboard, a novel online tool for visualization and multidimensional analysis of SHACL validation reports. It provides an interactive user interface featuring detailed violation summaries, analytical plots, and fine-grained insights into individual constraints. These functionalities enable users to efficiently understand validation results, identify problematic areas, and take precise corrective actions on their data.",
        spotlight: true,
    },
    {
        title: "MontoFlow – A Maritime Ontology Framework for Modeling Ship Sensory Systems",
        authors: "Pavle Ivanovic, Simon Burbach and Maria Maleshkova",
        abstract: "The increasing operational demands in maritime contexts, particularly during time-sensitive missions like search and rescue, necessitate reliable, intelligent support systems. These systems depend on semantically structured and interoperable models to integrate and interpret complex sensor data as well as facilitate informed decision-making. We introduce MontoFlow, a semantic integration framework that combines dynamic data access with domain-specific knowledge representation. It links static properties with dynamic sensory measurements, forming the foundation for advanced maritime diagnostics. At its core, MontoFlow incorporates the SHIP Ontology, a maritime-focused SSN/SOSA extension that provides a comprehensive semantic model describing onboard sensors, vessel components, and their observations. We illustrate the practical relevance and rationale behind the development of MontoFlow through real-world examples, with emphasis on ship maintenance and onboard anomaly detection. The SHIP Ontology is thoroughly evaluated based on domain coverage and a use case in the maritime context, demonstrating both high quality and practical applicability. This work presents a reusable and extensible resource for semantically enriching maritime sensory data, supporting advanced analytics and dynamic data monitoring."
    },
    {
        title: "MammoTab 25: A Large-Scale Dataset for Semantic Table Interpretation -- Training, Testing, and Detecting Weaknesses",
        authors: "Marco Cremaschi, Federico Belotti, Jennifer D'Souza and Matteo Palmonari",
        abstract: "The paper presents MammoTab 25, a new dataset comprising approximately 838k Wikipedia tables extracted from over 61M Wikipedia pages and semantically annotated through Wikidata. Each table in MammoTab 25 is accompanied by fine‑grained metadata, including column typing, NIL flags, and statistics, and by four prompt templates, making the resource simultaneously suitable for training, fine‑tuning, and stress‑testing Large Language Models (LLMs). MammoTab 25 covers, in a single benchmark, all key challenges for the semantic interpretation of tables, such as disambiguation issues, homonymy and acronym presence, NIL-mentions, and large web-table sizes; the tags attached to every table let researchers isolate and diagnose specific failure cases with precision. The corpus is delivered with an open‑source pipeline that can be re‑run on future Wikipedia dumps, ensuring long‑term sustainability and up‑to‑date annotations. MammoTab 25 already supports, and will continue to support, a public leaderboard that evaluates the STI capabilities of state‑of‑the‑art and upcoming LLMs, providing the community with a live yardstick of progress."
    },
    {
        title: "GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis",
        authors: "Ethan Frakes, Yinghui Wu, Roger French and Mengjie Li",
        abstract: "Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Numerous power outages occur each year, exacerbated by extreme weather events such as hurricanes and severe thunderstorms. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized outage patterns. However, it offers excellent temporal granularity, with updates available as frequently as every 15 minutes. In contrast, night-time light (NTL) satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events. However, these satellite data are only available on a daily basis. Integrating spatiotemporal visual and time-series data sources into a unified multimodal knowledge representation can substantially improve power outage detection, analysis, and predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning the source data with a developed ontology, GeoOutageOnto. Currently, GeoOutageKG includes over 10 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration. We demonstrate its use through multiresolution analysis of geospatiotemporal power outages. GeoOutageKG is available at \\url{https://github.com/UCF-HENAT/GeoOutageKG}. All image files and additional metadata are available at \\url{https://doi.org/10.17605/OSF.IO/QVD8B}."
    },
    {
        title: "gpuRDF2vec -- Scalable RDF2Vec using GPUs",
        authors: "Martin Böckling and Heiko Paulheim",
        abstract: "Generating Knowledge Graph (KG) embeddings at web scale remains challenging. Among existing techniques, RDF2vec combines effectiveness with strong scalability. We present gpuRDF2vec, an open source library that harnesses modern GPUs and supports multi-node execution to accelerate every stage of the RDF2vec pipeline. Extensive experiments on both synthetically generated graphs and real-world benchmarks show that gpuRDF2vec achieves up to a substantial speedup over the currently fastest alternative, i.e., jRDF2vec. In a single-node setup, our walk-extraction phase alone outperforms pyRDF2vec, SparkKGML, and jRDF2vec by a substantial margin using random walks on large/ dense graphs, and scales very well to longer walks, which typically lead to better quality embeddings. Our implementation of gpuRDF2vec enables practitioners and researchers to train high-quality KG embeddings on large-scale graphs within practical time budgets and builds on top of Pytorch Lightning for the scalable word2vec implementation."
    },
    {
        title: "Are LLMs Really Knowledgeable for Knowledge Graph Completion?",
        authors: "Yang Liu, Zequn Sun, Zhoutian Shao, Yuanning Cui and Wei Hu",
        abstract: "Knowledge Graph (KG) completion aims to infer new facts from existing knowledge. While recent efforts have explored leveraging large language models (LLMs) for this task, it remains unclear whether LLMs truly understand KG facts or how they utilize such knowledge in reasoning. In this work, we investigate these questions by proposing ProbeKGC,, a benchmark dataset that reformulates KG completion as multiple-choice question answering with systematically controlled option difficulties. Empirical results show that LLMs often produce inconsistent answers when the same question is presented with varying distractor difficulty, suggesting a reliance on shallow reasoning such as elimination rather than genuine knowledge recall. To better quantify model confidence and knowledge grasp, we introduce Normalized Knowledge Divergence (NKD), a novel metric that complements accuracy by capturing distributional confidence in answer selection. We further analyze the influence of selection biases on LLM predictions and highlight that LLMs do not always fully exploit their stored knowledge. Finally, we evaluate three enhancement strategies and provide insights into potential directions for improving KG completion.",
        spotlight: true
    }
];

export { resourceTrack };